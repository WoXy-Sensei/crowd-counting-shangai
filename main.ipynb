{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "g2lR_wjyQBmp"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import sklearn\n",
        "import cv2\n",
        "import scipy.io\n",
        "import os\n",
        "import tensorflow_probability as tfp\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.layers import Layer\n",
        "from tensorflow.keras.preprocessing.image import load_img, img_to_array\n",
        "from tensorflow.keras.applications import VGG16\n",
        "from tensorflow.keras.layers import (GlobalAveragePooling2D, Activation, MaxPooling2D, Add, Conv2D, MaxPool2D, Dense,\n",
        "                                     Flatten, InputLayer, BatchNormalization, Input, Embedding, Permute,\n",
        "                                     Dropout, RandomFlip, RandomRotation, LayerNormalization, MultiHeadAttention,\n",
        "                                     RandomContrast, Rescaling, Resizing, Reshape,LeakyReLU)\n",
        "from tensorflow.keras.losses import BinaryCrossentropy,CategoricalCrossentropy, SparseCategoricalCrossentropy\n",
        "from tensorflow.keras.metrics import Accuracy,TopKCategoricalAccuracy, CategoricalAccuracy, SparseCategoricalAccuracy\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.callbacks import (Callback, CSVLogger, EarlyStopping, LearningRateScheduler,\n",
        "                                        ModelCheckpoint, ReduceLROnPlateau)\n",
        "from tensorflow.keras.regularizers import L2, L1\n",
        "from tensorflow.keras.initializers import RandomNormal"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Download Dataset\n",
        "\n",
        "!gdown 13qzEsJJiSVcRiUDMx6C1E8xtiNo7hNUR"
      ],
      "metadata": {
        "id": "_V4nZS7Z4F3u"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Unzip Dataset\n",
        "\n",
        "!unzip shangai.zip"
      ],
      "metadata": {
        "id": "mAQt036l4c8b"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# DATA PREPARATION"
      ],
      "metadata": {
        "id": "kOZ4KwY3TbFa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "IN_X,IN_Y=768,1024\n",
        "OUT_X,OUT_Y=96,128\n",
        "SUBSAMPLING_FACTOR=IN_X//OUT_X"
      ],
      "metadata": {
        "id": "lxmP8WGGTZBU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def gauss_distribution(x,u=0,sigma=10):\n",
        "    return np.expand_dims(1/(np.sqrt(2*np.pi*(sigma**2)))*np.exp(-(0.5)*(((x-u)/sigma)**2)),axis=0)"
      ],
      "metadata": {
        "id": "ACZxPhNSTiyT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_density_map_gaussian(im,points,gaussian_radius=5):\n",
        "    density_map=np.zeros((OUT_X,OUT_Y))\n",
        "    w,h=OUT_Y,OUT_X\n",
        "    num_gt=len(points)\n",
        "\n",
        "    for point in points:\n",
        "        point=np.round(point).astype(int)\n",
        "        point[0],point[1]=min(h-1,point[1]),min(w-1,point[0])\n",
        "        x=np.linspace(-gaussian_radius,gaussian_radius,(gaussian_radius*2)+1)\n",
        "        gaussian_map=np.multiply(gauss_distribution(x),gauss_distribution(x).T)\n",
        "        gaussian_map/=np.sum(gaussian_map)\n",
        "\n",
        "        x_left,x_right,y_up,y_down=0,gaussian_map.shape[1],0,gaussian_map.shape[0]\n",
        "        if point[1]<gaussian_radius:\n",
        "            x_left=gaussian_radius-point[1]\n",
        "        if point[0]<gaussian_radius:\n",
        "            y_up=gaussian_radius-point[0]\n",
        "        if point[1]+gaussian_radius>=w:\n",
        "            x_right=gaussian_map.shape[1]-(gaussian_radius+point[1]-w)-1\n",
        "        if point[0]+gaussian_radius>=h:\n",
        "            y_down=gaussian_map.shape[0]-(gaussian_radius+point[0]-h)-1\n",
        "        density_map[\n",
        "            max(0,point[0]-gaussian_radius):min(density_map.shape[0],point[0]+gaussian_radius+1),\n",
        "            max(0,point[1]-gaussian_radius):min(density_map.shape[1],point[1]+gaussian_radius+1),\n",
        "        ]+=gaussian_map[y_up:y_down,x_left:x_right]\n",
        "    density_map/=np.sum(density_map/len(points))\n",
        "    return density_map"
      ],
      "metadata": {
        "id": "lfQg3N0eTZD1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class DataGenerator(tf.keras.utils.Sequence):\n",
        "    def __init__ (self, images, maps, batch_size,SUBSAMPLING_FACTOR=8):\n",
        "\n",
        "        self.images = images\n",
        "        self.maps = maps\n",
        "        self.batch_size = batch_size\n",
        "        self.train_image_list=os.listdir(images)\n",
        "        self.SUBSAMPLING_FACTOR=SUBSAMPLING_FACTOR\n",
        "\n",
        "    def __len__(self):\n",
        "        return int(np.floor(len(self.train_image_list)/self.batch_size))\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        x,y = self.__data_generation(idx)\n",
        "        return x,y\n",
        "\n",
        "    def __data_generation(self, idx):\n",
        "        x = []\n",
        "        y = []\n",
        "\n",
        "        for j in range(idx*self.batch_size, (idx+1)*self.batch_size):\n",
        "\n",
        "            im_array=img_to_array(load_img(self.images+os.listdir(self.images)[j],target_size=(IN_X,IN_Y)))\n",
        "            im_array/=255.\n",
        "            im_array[:,:,0]=(im_array[:,:,0]-np.mean(im_array[:,:,0]))/np.std(im_array[:,:,0])\n",
        "            im_array[:,:,1]=(im_array[:,:,1]-np.mean(im_array[:,:,1]))/np.std(im_array[:,:,1])\n",
        "            im_array[:,:,2]=(im_array[:,:,2]-np.mean(im_array[:,:,2]))/np.std(im_array[:,:,2])\n",
        "            x.append(im_array)\n",
        "            mat=scipy.io.loadmat(self.maps+os.listdir(self.maps)[j])\n",
        "            points=mat['image_info'][0][0][0][0][0]\n",
        "            points/=self.SUBSAMPLING_FACTOR\n",
        "\n",
        "            density_map_present=get_density_map_gaussian(im_array,points,5)\n",
        "            y.append(density_map_present)\n",
        "\n",
        "        return tf.convert_to_tensor(x),tf.convert_to_tensor(y)"
      ],
      "metadata": {
        "id": "IyICjrsjTZGl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_images='ShanghaiTech/part_A/train_data/images/'\n",
        "train_maps='ShanghaiTech/part_A/train_data/ground-truth/'\n",
        "val_images='ShanghaiTech/part_A/test_data/images/'\n",
        "val_maps='ShanghaiTech/part_A/test_data/ground-truth/'\n",
        "\n",
        "LR=1e-4\n",
        "BATCH_SIZE=4\n",
        "EPOCH=3"
      ],
      "metadata": {
        "id": "MEmDHPHrTnrr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_gen = DataGenerator(train_images, train_maps,BATCH_SIZE,SUBSAMPLING_FACTOR)"
      ],
      "metadata": {
        "id": "GLCoJNJrTnuN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# MODELING"
      ],
      "metadata": {
        "id": "E5Hzk0j4TqLM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_base_model():\n",
        "    base_model = VGG16(\n",
        "        weights='imagenet',\n",
        "        input_shape=(IN_X, IN_Y, 3),\n",
        "        include_top=False,\n",
        "    )\n",
        "\n",
        "    block4_conv3 = base_model.get_layer(\"block4_conv3\").output\n",
        "\n",
        "    return Model(\n",
        "        inputs=base_model.inputs, outputs=block4_conv3\n",
        "    )\n",
        "\n",
        "get_base_model().summary()\n"
      ],
      "metadata": {
        "id": "jfGjAqBVTnwV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "inputs = tf.keras.Input(shape=(None, None, 3))\n",
        "print(inputs.shape)\n",
        "x = get_base_model()(inputs)\n",
        "init = RandomNormal(stddev=0.01)\n",
        "\n",
        "x = Conv2D(512, (3, 3), activation='relu', dilation_rate=2, kernel_initializer=init, padding='same')(x)\n",
        "x = Conv2D(512, (3, 3), activation='relu', dilation_rate=2, kernel_initializer=init, padding='same')(x)\n",
        "x = Conv2D(512, (3, 3), activation='relu', dilation_rate=2, kernel_initializer=init, padding='same')(x)\n",
        "x = Conv2D(256, (3, 3), activation='relu', dilation_rate=2, kernel_initializer=init, padding='same')(x)\n",
        "x = Conv2D(128, (3, 3), activation='relu', dilation_rate=2, kernel_initializer=init, padding='same')(x)\n",
        "x = Conv2D(64, (3, 3), activation='relu', dilation_rate=2, kernel_initializer=init, padding='same')(x)\n",
        "x = Conv2D(1, (1, 1), activation='relu', dilation_rate=1, kernel_initializer=init, padding='same')(x)\n",
        "\n",
        "out = Reshape((96, 128))(x)\n",
        "\n",
        "model = tf.keras.Model(inputs=inputs, outputs=out)\n",
        "model.summary()\n"
      ],
      "metadata": {
        "id": "DPmTIVQ9Tnys"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def custom_loss(y_true,y_pred):\n",
        "  return tf.sqrt(tf.math.reduce_sum(tf.square(y_true - y_pred)))"
      ],
      "metadata": {
        "id": "QsjxehKYEM80"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.compile(\n",
        "    loss = custom_loss,\n",
        "    optimizer = Adam(learning_rate = LR),\n",
        "    metrics=['accuracy'],\n",
        "    run_eagerly = True,\n",
        ")"
      ],
      "metadata": {
        "id": "YT-kjG1JTZI-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "checkpoint_filepath='/model.weights.h5'\n",
        "callback = tf.keras.callbacks.ModelCheckpoint(\n",
        "    filepath = checkpoint_filepath,\n",
        "    save_weights_only=True,\n",
        "    monitor='loss',\n",
        "    mode='min',\n",
        "    save_best_only=True\n",
        ")"
      ],
      "metadata": {
        "id": "1QaRkOB0TyNl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "history = model.fit(\n",
        "    train_gen,\n",
        "    verbose=1,\n",
        "    shuffle=True,\n",
        "    epochs=EPOCH,\n",
        "    callbacks=[callback])"
      ],
      "metadata": {
        "id": "aCJsE-CCTyQC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.save('model.h5')"
      ],
      "metadata": {
        "id": "shRt6rvhSJQl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# TESTING"
      ],
      "metadata": {
        "id": "W8c53yULT1D7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "IN ='IMG_101'\n",
        "\n",
        "im_array=img_to_array(load_img(val_images+IN+'.jpg',target_size=(IN_X,IN_Y)))\n",
        "im_array/=255.\n",
        "im_array[:,:,0]=(im_array[:,:,0]-np.mean(im_array[:,:,0]))/np.std(im_array[:,:,0])\n",
        "im_array[:,:,1]=(im_array[:,:,1]-np.mean(im_array[:,:,1]))/np.std(im_array[:,:,1])\n",
        "im_array[:,:,2]=(im_array[:,:,2]-np.mean(im_array[:,:,2]))/np.std(im_array[:,:,2])\n",
        "\n",
        "plt.figure(figsize=(20,12))\n",
        "plt.imshow(im_array)\n",
        "\n",
        "output=model.predict(tf.expand_dims(im_array,axis=0))\n",
        "output=np.reshape(output,(OUT_X,OUT_Y))\n",
        "\n",
        "n_people=np.sum(output)\n",
        "mat=scipy.io.loadmat(train_maps+'GT_'+IN+'.mat')\n",
        "points=mat['image_info'][0][0][0][0][0]\n",
        "points/=SUBSAMPLING_FACTOR\n",
        "\n",
        "num_gt=np.squeeze(points).shape[0]\n",
        "print(\"The actual number of people is=\",num_gt)\n",
        "print(\"The predicted number of people is =\",n_people)\n",
        "\n",
        "plt.figure(figsize=(20,12))\n",
        "plt.imshow(output)"
      ],
      "metadata": {
        "id": "GrVfcr8OT0YF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "jOIrzCOgT0aj"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}